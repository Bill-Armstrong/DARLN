An ALN for deep learning has a very simple functional form.  It is composed of affine functions defining planes, or in higher dimensions, hyperplanes, and maximum and minimum operators to combine the affine functions. So the graph of the function learned is composed of flat pieces joined to form a continuous surface.  Suppose now the function to be learned from samples indicates the value of a sequence of actions each of which is associated with a reinforcement (or punishment). If that function can predict the likely value of performing a sequence of actions, then the question becomes: "Can one change the inputs to improve the predicted value by changing the actions involved?" Since any input vector selects one flat part of the function surface, a path towards greater predicted reinforcement value is obtained by examining the coefficients of that flat piece. Following this path, perhaps along several flat pieces of the function surface, the inputs, e.g. proposed effector positions of a robot, can be changed to increase the value.

We are a long way from the final realization of this method, but the deep learning method presented in Deep-Learning-ALN is a start. Probably the first step someone should try is training ALNs on complicated, high-dimensional (lots of sensors and effectors) tasks and
developing techniques to make training even faster.  Ideas like alpha-beta pruning are used already in the program. Training could be localized too. Just like the DTREE idea partitions the input space so huge ALNs can be evaluated by quickly selecting one of a collection of much smaller ALNs to be evaluated. Locality and speed of computation is a distinct benefit of this kind of multilayer perceptron over the ones using standard back-propagation.

A simple piece of the puzzle is how to simplify functions.  One can throw out flat pieces that are not being used. Consider a flat piece that lies above the minimum of two pieces forming a tent-like surface below it.  In a minimum together with the tent, that piece is useless. Further improvements come by limiting the number of variables entering into each affine function.  The "importance" values computed in ALNfitDeep select input variables for all affine functions in the whole ALN.  In general, different parts of a huge ALN could depend on different input variables. This is not the case for ALNfitDeep.

Learning the value of reinforcements based on, for example, visual sensor data that arise from the same object with varying  positions, sizes and orientations could vastly expand the domain of the reinforcement function. Position can be treated by using a shift-register approach to feeding the data into the reinforcement network.

Several facts point to this plan for advancing reinforcement learning being realizable:

1. ALNs can be grown to fit any continuous function to any desired accuracy uniformly on any compact domain.

2. If an ALN is prescribed to be strongly monotonic in any input variable, then without further training, an ALN can be constructed to compute the inverse function by simple changes of coefficients and maximum and minimum operators.

3. Value iteration and Q-learning have been shown to work on problems like the inverted pendulum. This is probably due to the fact that the value function, for example, can be as "fine-grained" as desired, so Bellman's equation, where a function value is determined by those close to it, can be solved. Having an architecture which can grow without limit is important here.

That's the general idea.  Please work on these ideas. Publish papers and make your software available to others. I look forward to your collaboration, and I will try to help you where I can.

I wish us luck.

Bill Armstrong

wwarmstrong
AT
shaw.ca 